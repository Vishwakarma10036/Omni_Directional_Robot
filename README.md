# Omnidirectional Visionâ€‘Guided Autonomous Robot with Object Recognition and Navigation (ROSÂ 2)

## ğŸ“Œ Project Overview

This project presents the design and implementation of an **omnidirectional autonomous mobile robot** equipped with **Mecanum wheels**, **visionâ€‘based perception**, **ultrasonic sensing**, and a **ROSÂ 2 navigation stack**. The robot is capable of:

* Omnidirectional motion using Mecanum wheel kinematics
* Visionâ€‘guided object recognition and tracking
* Autonomous navigation with global & local planning
* Realâ€‘time obstacle avoidance in dynamic environments
* Hybrid planning using **RRT + D* Lite**
* Manual and autonomous mode switching

The system is designed with a **modular ROSÂ 2 architecture**, enabling scalability, reusability, and easy integration of additional sensors or algorithms.

---

## ğŸ¯ Key Features

* **Omnidirectional Mobility** â€“ Smooth holonomic motion (forward, sideways, diagonal, rotation)
* **ROSÂ 2 (Nav2 Stack)** â€“ Localization, mapping, and navigation
* **Visionâ€‘Based Object Recognition** â€“ Cameraâ€‘based perception pipeline
* **Hybrid Path Planning** â€“ RRT for global path, D* Lite for dynamic replanning
* **Reactive Obstacle Avoidance** â€“ Ultrasonic sensorâ€“based safety layer
* **Costmapâ€‘Aware Navigation** â€“ Dynamic obstacle updates
* **Manual + Autonomous Modes** â€“ Priorityâ€‘based command fusion
* **Realâ€‘Time Feedback** â€“ Sensor data and camera feed visualization

---

## ğŸ§  System Architecture

### Highâ€‘Level Architecture

The system is divided into the following subsystems:

1. **Perception Subsystem**

   * Camera processing (object detection, tracking)
   * Ultrasonic distance sensing
   * Noise filtering and thresholding

2. **Planning Subsystem**

   * Global planner (RRT)
   * Incremental replanning (D* Lite)
   * Local planner (Nav2)

3. **Decision & Control Subsystem**

   * Behavior decision engine
   * Command fusion and priority resolver
   * Mecanum kinematic controller

4. **Execution Subsystem**

   * Motor actuation
   * Servo camera control

---

## ğŸ” Autonomous Robot Flow

1. System start & hardware initialization
2. ROSÂ 2 nodes and parameters loaded
3. Sensors, camera, and SLAM nodes activated
4. User selects **Manual** or **Autonomous** mode
5. Autonomous stack initializes planners
6. Continuous perception â†’ planning â†’ control loop
7. Robot navigates until goal is reached or stop signal received

---

## ğŸ—ºï¸ Path Planning Logic (RRT + D* Lite)

### Stepâ€‘byâ€‘Step Flow

* Initialize environment map
* Take start and goal positions
* Generate **global path using RRT**
* Load RRT path into **D* Lite**
* Robot begins motion along the path
* Continuously sense environment
* If environment unchanged â†’ follow path
* If obstacle detected â†’ D* Lite updates only affected edges
* Extract updated path
* Mecanum controller follows the new trajectory
* Repeat until goal is reached

### Why RRT + D* Lite?

* **RRT** efficiently handles large, unknown spaces
* **D* Lite** enables fast replanning without recomputing the entire graph
* Ideal for **dynamic and partially known environments**

---

## ğŸš§ Obstacle Avoidance Algorithm

### Thresholdâ€‘Based Reactive Avoidance

1. Activate ultrasonic sensor
2. Continuously measure distance ( d(t) )
3. Collect multiple samples
4. Apply noise filtering (mean + standard deviation)
5. Remove outliers
6. Compare filtered distance with threshold ( T )
7. If ( d(t) \le T ): trigger avoidance (stop / turn)
8. If ( d(t) > T ): move forward
9. Update wheel velocities using Mecanum kinematics
10. Repeat sensingâ€“decisionâ€“action loop

This reactive layer runs **in parallel** with the navigation stack, acting as a safety override.

---

## ğŸ“· Visionâ€‘Based Perception

* Camera mounted on servo for dynamic tracking
* Image processing pipeline:

  * Frame acquisition
  * Object detection / color tracking
  * Feature extraction
  * Bounding box & centroid estimation
* Vision data used for:

  * Obstacle awareness
  * Object following
  * User feedback overlay

---

## âš™ï¸ Mecanum Wheel Kinematics

The robot uses **holonomic motion equations** to independently control each wheel:

* Allows translation in X, Y, and rotation simultaneously
* Wheel velocities computed from:

  * Linear velocity (vx, vy)
  * Angular velocity (Ï‰)

This enables precise navigation in narrow and cluttered environments.

---

## ğŸ§© ROSÂ 2 Node Structure

```
ros2_ws/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ perception_pkg/
â”‚   â”‚   â”œâ”€â”€ camera_node.py
â”‚   â”‚   â”œâ”€â”€ ultrasonic_node.cpp
â”‚   â”‚   â””â”€â”€ sensor_fusion_node.py
â”‚   â”œâ”€â”€ planning_pkg/
â”‚   â”‚   â”œâ”€â”€ rrt_planner.cpp
â”‚   â”‚   â”œâ”€â”€ dstar_lite.cpp
â”‚   â”‚   â””â”€â”€ path_manager.cpp
â”‚   â”œâ”€â”€ control_pkg/
â”‚   â”‚   â”œâ”€â”€ mecanum_controller.cpp
â”‚   â”‚   â””â”€â”€ motor_driver_node.cpp
â”‚   â”œâ”€â”€ decision_pkg/
â”‚   â”‚   â””â”€â”€ behavior_engine.cpp
â”‚   â””â”€â”€ bringup_pkg/
â”‚       â””â”€â”€ launch/
â”‚           â””â”€â”€ robot_launch.py
```

---

## ğŸ”„ Command Fusion & Priority Resolver

Command priority order:

1. Emergency stop
2. Obstacle avoidance override
3. Manual control
4. Autonomous navigation

This ensures **safe and predictable robot behavior** under all conditions.

---

## ğŸ› ï¸ Hardware Components

* Mecanum wheel chassis (4â€‘wheel)
* DC geared motors
* Motor driver (highâ€‘current)
* Ultrasonic sensor
* Camera module
* Servo motor (camera pan)
* Embedded controller (Raspberry Pi / MCU)
* Battery & power management

---

## ğŸ’» Software Stack

* **ROSÂ 2 (Humble / Foxy)**
* Nav2 Navigation Stack
* OpenCV
* Python & C++
* SLAM Toolbox
* RViz2
* Linux (Ubuntu)

---

## â–¶ï¸ How to Run

```bash
# Build workspace
colcon build

# Source setup
source install/setup.bash

# Launch robot system
ros2 launch bringup_pkg robot_launch.py
```

---

## ğŸ“Š Results & Performance

* Accurate omnidirectional motion
* Reliable obstacle avoidance in dynamic environments
* Fast replanning with minimal latency
* Stable navigation with continuous costmap updates

---

## ğŸš€ Future Improvements

* LiDARâ€‘based perception
* 3D SLAM integration
* Deepâ€‘learning object detection
* Multiâ€‘robot coordination
* Autonomous docking & charging

---


## ğŸ“œ License

This project is released under the **MIT License**.

---

â­ *If you find this project useful, please star the repository!*
